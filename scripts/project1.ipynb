{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cross validation for training, return the optimal weigths and theirs respective loss for the train and the test datas\n",
    "def cross_validation(y, tx, k_indices, k, lambda_, degree):\n",
    "    \n",
    "    # get k'th subgroup in test, others in train\n",
    "    training_indices = k_indices[~(np.arange(len(k_indices)) == k)].reshape(-1)\n",
    "    test_indices = k_indices[k]\n",
    "    \n",
    "    tx_train = tx[training_indices]\n",
    "    tx_test = tx[test_indices]\n",
    "    y_train = y[training_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # features expansion\n",
    "    tx_train = build_poly(tx_train, degree)\n",
    "    tx_test = build_poly(tx_test, degree)\n",
    "    \n",
    "    # optimization with ridge_regression\n",
    "    weigths = ridge_regression(y_train, tx_train, lambda_)\n",
    "    \n",
    "    # compute the loss for the train and test datas with the weigths found\n",
    "    loss_train = compute_mse(y_train, tx_train, weigths)\n",
    "    loss_test = compute_mse(y_test, tx_test, weigths)\n",
    "    \n",
    "    return weigths, loss_train, loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the best hyperparameters for regularized optimization\n",
    "def best_hyperparameters(y, tx, degrees, lambdas, k_fold, seed=1):\n",
    "    # for each degree, store the best lambda and the respective loss\n",
    "    losses = []\n",
    "    lambdas_best = []\n",
    "    \n",
    "    # build k indices for k-fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # compute cross validation with all lambdas for each degree\n",
    "    for degree in degrees:\n",
    "        \n",
    "        # store the loss, respective to the lambdas\n",
    "        losses_test = []\n",
    "        \n",
    "        # compute cross validation for each lambda of the specific degree\n",
    "        for lambda_ in lambdas:\n",
    "        \n",
    "            # to compute the total loss of each lambda by storing the loss for each iteration \n",
    "            # of the k-fold and computing the mean\n",
    "            losses_test_tmp = []\n",
    "        \n",
    "            # compute loss for each iteration of the k_fold\n",
    "            for k in range(k_fold):\n",
    "                weigths, loss_train, loss_test = cross_validation(y, tx, k_indices, k, lambda_, degree)\n",
    "                losses_test_tmp.append(loss_test)\n",
    "            \n",
    "            \n",
    "            #compute the loss for the specific lambda by taking the mean of the losses of each iteration of the k-fold\n",
    "            losses_test.append(np.mean(losses_test_tmp))\n",
    "        \n",
    "        # find the optimal lambda hyperparameter by getting the minimum loss for each degree\n",
    "        best_lambda_index = np.argmin(losses_test)\n",
    "        lambdas_best.append(lambdas[best_lambda_index])\n",
    "        losses.append(losses_test[best_lambda_index])\n",
    "        \n",
    "    # find the optimal degree hyperparameter by getting the minimum loss\n",
    "    best_degree_index = np.argmin(losses)\n",
    "    \n",
    "    # compute the optimal hyperparameters\n",
    "    opt_degree = degrees[best_degree_index]\n",
    "    opt_lambda = lambdas_best[best_degree_index]\n",
    "    \n",
    "    \n",
    "    return opt_degree, opt_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the best hyperparameters for regularized optimization for each subset of the training dataset\n",
    "#Â and return the best weights of each subset, respective to the best hyperparameters\n",
    "def train_models(y, tx, degrees, lambdas, k_fold, seed=1):\n",
    "    \n",
    "    # get the indices of each training subset\n",
    "    indices_group = group_indices(tx)\n",
    "    \n",
    "    # store the best weights and degree for each training subset\n",
    "    best_weights = []\n",
    "    best_degree = []\n",
    "    \n",
    "    # compute the optimal hyperparameters for each training subset and the respective weights\n",
    "    for indice_group in indices_group:\n",
    "        y_subset = y[indice_group]\n",
    "        tx_subset = drop_na_columns(tx[indice_group])\n",
    "        \n",
    "        opt_degree, opt_lambda = best_hyperparameters(y_subset, tx_subset, degrees, lambdas, k_fold, seed)\n",
    "        weights = ridge_regression(y_subset, build_poly(tx_subset, opt_degree), opt_lambda)\n",
    "        \n",
    "        best_degree.append(opt_degree)\n",
    "        best_weights.append(weights)\n",
    "        \n",
    "    return best_weights, best_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = np.where(tX==-999, np.NaN, tX)\n",
    "best_weights, best_degree = train_models(y, tX, np.arange(5), np.logspace(-4, 0, 20), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 2, 3, 3, 4]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568238"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_test = np.where(tX_test==-999, np.NaN, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([     0,     14,     15, ..., 568193, 568218, 568220]),),\n",
       " (array([     2,      3,      5, ..., 568232, 568235, 568237]),),\n",
       " (array([    34,     37,     49, ..., 568180, 568214, 568224]),),\n",
       " (array([     1,      6,     10, ..., 568227, 568231, 568236]),),\n",
       " (array([    35,    173,    201, ..., 568075, 568189, 568223]),),\n",
       " (array([     4,      7,      9, ..., 568230, 568233, 568234]),)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_test_group = group_indices(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1., ...,  1., -1., -1.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.zeros(tX_test.shape[0])\n",
    "for i, indice_test_group in enumerate(indices_test_group):\n",
    "    y_pred_subset = predict_labels(best_weights[i], build_poly(drop_na_columns(tX_test[indice_test_group]), best_degree[i]))\n",
    "    y_pred[indice_test_group] = y_pred_subset\n",
    "    \n",
    "    \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/sample-submission_test.csv' \n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
